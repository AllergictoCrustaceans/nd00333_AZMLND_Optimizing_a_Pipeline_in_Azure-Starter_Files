# Optimizing an ML Pipeline in Azure

I am still learning AzureML (let alone general ML), and ran into a lot of hiccups throughout the project. 
I had to google/github/stackoverflow for some of my answers in this project. In honor of the honor system, I would like to list out who/where I got help to debug/solve my issues:
- [Sanity/debug check from this guy-- thank you](https://github.com/QuirkyDataScientist1978/Microsoft-Azure-Machine-Learning-Engineer-Project-1-Udacity-Solution)
- [How to save a model](https://github.com/MicrosoftDocs/azure-docs/issues/45773)

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains data about subjects from various socio-economic backgrounds, and we seek to predict whether these subjects would agree to take part in a bank's marketing campaign. We choose to tackle this ML problem through logistic regression and classification. 

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was with AutoML, where the VotingEnsemble pipeline (iteration 24) yielded an accuracy of 91.6%. 
The accuracy from this autoML model was 0.4% higher than that of the best run from the logistic regression model w/ HyperDrive tuning (which yielded an accuracy of 91.2%). 
Although autoML yielded a slightly better result, the difference in accuracy between the two models may not be extremely significant. 


## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The SKLearn pipline uses 'train.py' as its entry script, logistic regression as its ML model of choice, and HyperDrive for its hyperparameter tuning. 
### What train.py handles in the pipeline: 
- Function `clean_data` is defined. It cleans the raw data by dropping NA values, dropping unwanted volumns (e.g. job, contact, education), one-hot-encoding categorical values in certain columns (e.g. marital, default, housing, loan, poutcome), and one-hot-encoding the y variable (e.g. column named 'y').
- Function `main` is defined. It imports raw data via `TabularDatasetFactory.from_delimited_files()` method, called the `clean_data()` to clean the raw data, splits the data into train/test distributions via `sklearn.model_selection.train_test_split()`, creates a Logistic Regression model with paramters C and max_iter., fits the training dataset to it, and predicts the accuracy of the model by passing in the testing dataset.  

### What does HyperDrive handle in the pipeline:
- Within the Jupyter notebook, the HyperDrive configurations are defined. The parameter sampler, primary metric, early stopping policy, max number of runs, and estimator are passed in here.

### What else is a part of the pipeline:
- Run the hyperdrive model via calling `exp.submit()` and passing in the hyperdrive configurations into the method. 
- After training and testing the model, we call `get_best_run_by_primary_metric` to fetch the best run model, and `register_model()` to save the best run model. 

**What are the benefits of the parameter sampler you chose?**
I chose to use the default sampler that was already imported into the notebook cell, which was the RandomParameterSampling sampler. Since this sampler enables random selection of hyperparameters (discrete and/or continuous), this allows us to assess a wide range of values that would help yield the best performing model. 

**What are the benefits of the early stopping policy you chose?**
I chose to use the BanditPolicy early stopping policy, which was already imported by deault into the notebook. This policy terminates any runs where the primary metric (e.g. accuracy) does not meet the bounds of the best performing run's slack factor. The BanditPolicy ensures us to return models that are better than the last best performing one, and would not waste time running iterations on one that isn't performing as well.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML generates multiple models throughout its run. It's configured to do classification for no more than the allotted time (which was 30min in our case), with accuracy as its primary metric, and it's enabled for early stopping. Most of the models its generated its best metric at 91.48%, with Voting Ensemble as the only model yielding a 91.64%.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
In terms of accuracy, (also -- as stated in the `Summary` section of this README.md) the AutoML's best model came from the VotingEnsemble pipeline, yielding an accuracy of 91.6%, compared to that of logistic regression w/ HyperDrive tuning which yielded an accuracy of 91.2%. In terms of architecture, the SKLearn pipeline only trains and tests its data on logistic regression while AutoML trains and tests its data on multiple models. Overall, I am unsure how significant a 0.4% difference is between the two models. However, since AutoML results faired better, I am assuming it is because of its ability to assess multiple models' accuracy in a short amount of time, rather than just logistic regression.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Collecting more data, and mitigating class imbalance would help reduce bias in the model, therefore producing a model that can predict better.  
AutoML is killer. I assume with longer runtimes, and possibly with a bigger dataset, it would do a more comprehensive assessment of which models yield better results. 

## Proof of cluster clean up
Cluster cleanup code 
I did not include the cluster cleanup code in the notebook, so I am completing it here in this section.
```
compute_target.delete()
```
Since I did not run it in the notebook, I do not have an image of the cluster marked for deletion.
